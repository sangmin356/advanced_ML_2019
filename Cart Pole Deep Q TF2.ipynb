{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cart Pole with neural function approximator TF2\n",
    "### Christian Igel, 2019\n",
    "\n",
    "If you have suggestions for improvement, [let me know](mailto:igel@diku.dk).\n",
    "\n",
    "I took inspiration from https://github.com/udacity/deep-learning/blob/master/reinforcement/Q-learning-cart.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Cart-Pole game environment\n",
    "env = gym.make('CartPole-v0')\n",
    "action_size = 2\n",
    "state_size = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just test the environment first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial state: [ 0.04955984 -0.00537115 -0.00052219 -0.04178765]\n",
      "return:  10.0\n",
      "initial state: [-0.02170706  0.0149275   0.02481736 -0.01341253]\n",
      "return:  16.0\n",
      "initial state: [ 0.0451842  -0.00534536 -0.02612767  0.00211796]\n",
      "return:  41.0\n",
      "initial state: [-0.01677121 -0.02224579 -0.0065124   0.00643503]\n",
      "return:  20.0\n",
      "initial state: [ 0.02683571 -0.04734673  0.03977117  0.02596666]\n",
      "return:  26.0\n",
      "initial state: [ 0.03973656  0.00048963  0.0226608  -0.02144592]\n",
      "return:  60.0\n",
      "initial state: [-0.03065499 -0.03581875  0.03972057  0.0334654 ]\n",
      "return:  13.0\n",
      "initial state: [ 0.02185343  0.03852237 -0.03194285 -0.02407094]\n",
      "return:  24.0\n",
      "initial state: [-0.01823235 -0.03445122  0.01056239  0.01591504]\n",
      "return:  15.0\n",
      "initial state: [-0.0050449  -0.03697841 -0.0061942  -0.04961251]\n",
      "return:  33.0\n"
     ]
    }
   ],
   "source": [
    "test_episodes = 10\n",
    "for _ in range(test_episodes):\n",
    "    R = 0\n",
    "    state = env.reset()  # Environment starts in a random state, cart and pole are moving\n",
    "    print(\"initial state:\", state)\n",
    "    while True:  # Environment sets \"done\" to true after 200 steps \n",
    "        # Uncomment the line below to watch the simulation\n",
    "        # env.render()\n",
    "        state, reward, done, info = env.step(env.action_space.sample()) # take a random action\n",
    "        R += reward\n",
    "        if done:\n",
    "            print(\"return: \", R)\n",
    "            env.reset()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()  # Closes the visualization window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define *Q* network architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(tf.keras.Model):\n",
    "    def __init__(self, state_size=4, action_size=2, hidden_size=10, name='QNetwork'):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = tf.keras.layers.Dense(hidden_size, activation='relu', use_bias=True)\n",
    "        self.fc2 = tf.keras.layers.Dense(hidden_size, activation='relu', use_bias=True)\n",
    "        self.fc3 = tf.keras.layers.Dense(action_size, activation=None, use_bias=True)\n",
    "    def call(self, x):\n",
    "        return self.fc3(self.fc2(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data structure for storing experiences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), \n",
    "                               size=batch_size, \n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define basic constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_episodes = 400           # max number of episodes to learn from\n",
    "max_steps = 200                # max steps in an episode\n",
    "gamma = 0.99                   # future reward discount\n",
    "\n",
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.005           # minimum exploration probability \n",
    "decay_rate = 0.001             # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "hidden_size = 64               # number of units in each Q-network hidden layer\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = 10000            # memory capacity\n",
    "batch_size = 100               # experience mini-batch size\n",
    "pretrain_length = batch_size   # number experiences to pretrain the memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"q_network\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                multiple                  320       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  4160      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  130       \n",
      "=================================================================\n",
      "Total params: 4,610\n",
      "Trainable params: 4,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "mainQN = QNetwork(name='main', hidden_size=hidden_size)\n",
    "mainQN.build(input_shape=(None, state_size))\n",
    "print(mainQN.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the *Q*-learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the simulation\n",
    "env.reset()\n",
    "# Take one random step to get the pole and cart moving\n",
    "state, reward, done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "memory = Memory(max_size=memory_size)\n",
    "\n",
    "# Make a bunch of random actions and store the experiences\n",
    "for _ in range(pretrain_length):\n",
    "    # Uncomment the line below to watch the simulation\n",
    "    # env.render()\n",
    "\n",
    "    # Make a random action\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "    if done:\n",
    "        # The simulation fails, so no next state\n",
    "        next_state = np.zeros(state.shape)\n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state))\n",
    "        \n",
    "        # Start new episode\n",
    "        env.reset()\n",
    "        # Take one random step to get the pole and cart moving\n",
    "        state, reward, done, _ = env.step(env.action_space.sample())\n",
    "    else:\n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state))\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001         # Q-network learning rate\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=0.01)\n",
    "\n",
    "@tf.function\n",
    "def compute_gradient(states, actions, targets, action_size=2):\n",
    "    one_hot_actions = tf.one_hot(actions, action_size)\n",
    "    with tf.GradientTape() as tape:\n",
    "        output = mainQN(states)\n",
    "        Q = tf.reduce_sum(tf.multiply(output, one_hot_actions), axis=1)\n",
    "        loss = tf.math.reduce_mean(tf.square(Q - targets))\n",
    "    return loss, tape.gradient(loss, mainQN.trainable_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train with experiences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Total reward: 36.0 Training loss: 1.0270 Explore P: 0.9648\n",
      "Episode: 1 Total reward: 34.0 Training loss: 1.0229 Explore P: 0.9327\n",
      "Episode: 2 Total reward: 13.0 Training loss: 1.0345 Explore P: 0.9207\n",
      "Episode: 3 Total reward: 16.0 Training loss: 1.0205 Explore P: 0.9062\n",
      "Episode: 4 Total reward: 13.0 Training loss: 1.0343 Explore P: 0.8946\n",
      "Episode: 5 Total reward: 17.0 Training loss: 1.0120 Explore P: 0.8796\n",
      "Episode: 6 Total reward: 41.0 Training loss: 1.0462 Explore P: 0.8444\n",
      "Episode: 7 Total reward: 22.0 Training loss: 1.0622 Explore P: 0.8262\n",
      "Episode: 8 Total reward: 20.0 Training loss: 1.0862 Explore P: 0.8099\n",
      "Episode: 9 Total reward: 24.0 Training loss: 0.9847 Explore P: 0.7908\n",
      "Episode: 10 Total reward: 14.0 Training loss: 1.0369 Explore P: 0.7799\n",
      "Episode: 11 Total reward: 21.0 Training loss: 1.0572 Explore P: 0.7638\n",
      "Episode: 12 Total reward: 11.0 Training loss: 1.1239 Explore P: 0.7555\n",
      "Episode: 13 Total reward: 18.0 Training loss: 1.1293 Explore P: 0.7421\n",
      "Episode: 14 Total reward: 14.0 Training loss: 1.0957 Explore P: 0.7319\n",
      "Episode: 15 Total reward: 15.0 Training loss: 1.1768 Explore P: 0.7210\n",
      "Episode: 16 Total reward: 12.0 Training loss: 1.1856 Explore P: 0.7125\n",
      "Episode: 17 Total reward: 10.0 Training loss: 1.1789 Explore P: 0.7055\n",
      "Episode: 18 Total reward: 12.0 Training loss: 1.2712 Explore P: 0.6971\n",
      "Episode: 19 Total reward: 13.0 Training loss: 1.2481 Explore P: 0.6882\n",
      "Episode: 20 Total reward: 11.0 Training loss: 1.2689 Explore P: 0.6807\n",
      "Episode: 21 Total reward: 17.0 Training loss: 1.3224 Explore P: 0.6693\n",
      "Episode: 22 Total reward: 9.0 Training loss: 1.3332 Explore P: 0.6634\n",
      "Episode: 23 Total reward: 9.0 Training loss: 1.4179 Explore P: 0.6575\n",
      "Episode: 24 Total reward: 10.0 Training loss: 1.3793 Explore P: 0.6510\n",
      "Episode: 25 Total reward: 14.0 Training loss: 1.4767 Explore P: 0.6420\n",
      "Episode: 26 Total reward: 10.0 Training loss: 1.4409 Explore P: 0.6356\n",
      "Episode: 27 Total reward: 10.0 Training loss: 1.5146 Explore P: 0.6294\n",
      "Episode: 28 Total reward: 14.0 Training loss: 1.6724 Explore P: 0.6207\n",
      "Episode: 29 Total reward: 13.0 Training loss: 1.6512 Explore P: 0.6127\n",
      "Episode: 30 Total reward: 11.0 Training loss: 1.9323 Explore P: 0.6061\n",
      "Episode: 31 Total reward: 14.0 Training loss: 1.9223 Explore P: 0.5977\n",
      "Episode: 32 Total reward: 11.0 Training loss: 1.8551 Explore P: 0.5912\n",
      "Episode: 33 Total reward: 16.0 Training loss: 2.1895 Explore P: 0.5819\n",
      "Episode: 34 Total reward: 16.0 Training loss: 1.9869 Explore P: 0.5728\n",
      "Episode: 35 Total reward: 9.0 Training loss: 2.0580 Explore P: 0.5677\n",
      "Episode: 36 Total reward: 16.0 Training loss: 3.2287 Explore P: 0.5588\n",
      "Episode: 37 Total reward: 9.0 Training loss: 2.1703 Explore P: 0.5538\n",
      "Episode: 38 Total reward: 13.0 Training loss: 2.2792 Explore P: 0.5467\n",
      "Episode: 39 Total reward: 10.0 Training loss: 3.1971 Explore P: 0.5413\n",
      "Episode: 40 Total reward: 9.0 Training loss: 3.0333 Explore P: 0.5365\n",
      "Episode: 41 Total reward: 15.0 Training loss: 3.3915 Explore P: 0.5286\n",
      "Episode: 42 Total reward: 13.0 Training loss: 4.4331 Explore P: 0.5218\n",
      "Episode: 43 Total reward: 11.0 Training loss: 4.2148 Explore P: 0.5162\n",
      "Episode: 44 Total reward: 20.0 Training loss: 3.1538 Explore P: 0.5061\n",
      "Episode: 45 Total reward: 13.0 Training loss: 6.0318 Explore P: 0.4996\n",
      "Episode: 46 Total reward: 11.0 Training loss: 4.9680 Explore P: 0.4942\n",
      "Episode: 47 Total reward: 14.0 Training loss: 4.2297 Explore P: 0.4874\n",
      "Episode: 48 Total reward: 12.0 Training loss: 4.6935 Explore P: 0.4816\n",
      "Episode: 49 Total reward: 11.0 Training loss: 3.7671 Explore P: 0.4764\n",
      "Episode: 50 Total reward: 11.0 Training loss: 5.5169 Explore P: 0.4713\n",
      "Episode: 51 Total reward: 12.0 Training loss: 4.5376 Explore P: 0.4657\n",
      "Episode: 52 Total reward: 12.0 Training loss: 4.7451 Explore P: 0.4602\n",
      "Episode: 53 Total reward: 11.0 Training loss: 6.9826 Explore P: 0.4552\n",
      "Episode: 54 Total reward: 11.0 Training loss: 8.2305 Explore P: 0.4503\n",
      "Episode: 55 Total reward: 9.0 Training loss: 5.3734 Explore P: 0.4463\n",
      "Episode: 56 Total reward: 10.0 Training loss: 5.9502 Explore P: 0.4419\n",
      "Episode: 57 Total reward: 15.0 Training loss: 9.0285 Explore P: 0.4354\n",
      "Episode: 58 Total reward: 13.0 Training loss: 4.6460 Explore P: 0.4299\n",
      "Episode: 59 Total reward: 10.0 Training loss: 8.2268 Explore P: 0.4256\n",
      "Episode: 60 Total reward: 15.0 Training loss: 10.1463 Explore P: 0.4194\n",
      "Episode: 61 Total reward: 11.0 Training loss: 12.3013 Explore P: 0.4148\n",
      "Episode: 62 Total reward: 10.0 Training loss: 10.8830 Explore P: 0.4108\n",
      "Episode: 63 Total reward: 9.0 Training loss: 10.4808 Explore P: 0.4071\n",
      "Episode: 64 Total reward: 11.0 Training loss: 13.4527 Explore P: 0.4027\n",
      "Episode: 65 Total reward: 14.0 Training loss: 11.3013 Explore P: 0.3972\n",
      "Episode: 66 Total reward: 12.0 Training loss: 14.1847 Explore P: 0.3925\n",
      "Episode: 67 Total reward: 12.0 Training loss: 8.8525 Explore P: 0.3879\n",
      "Episode: 68 Total reward: 16.0 Training loss: 7.1071 Explore P: 0.3818\n",
      "Episode: 69 Total reward: 11.0 Training loss: 13.6517 Explore P: 0.3777\n",
      "Episode: 70 Total reward: 10.0 Training loss: 8.8317 Explore P: 0.3740\n",
      "Episode: 71 Total reward: 11.0 Training loss: 11.2448 Explore P: 0.3699\n",
      "Episode: 72 Total reward: 10.0 Training loss: 12.6635 Explore P: 0.3663\n",
      "Episode: 73 Total reward: 15.0 Training loss: 11.9866 Explore P: 0.3609\n",
      "Episode: 74 Total reward: 13.0 Training loss: 10.7900 Explore P: 0.3563\n",
      "Episode: 75 Total reward: 16.0 Training loss: 21.2303 Explore P: 0.3508\n",
      "Episode: 76 Total reward: 17.0 Training loss: 4.8683 Explore P: 0.3449\n",
      "Episode: 77 Total reward: 9.0 Training loss: 9.1256 Explore P: 0.3419\n",
      "Episode: 78 Total reward: 9.0 Training loss: 17.6783 Explore P: 0.3389\n",
      "Episode: 79 Total reward: 8.0 Training loss: 10.0167 Explore P: 0.3362\n",
      "Episode: 80 Total reward: 9.0 Training loss: 23.5270 Explore P: 0.3332\n",
      "Episode: 81 Total reward: 11.0 Training loss: 14.7206 Explore P: 0.3296\n",
      "Episode: 82 Total reward: 11.0 Training loss: 15.9200 Explore P: 0.3261\n",
      "Episode: 83 Total reward: 10.0 Training loss: 20.4617 Explore P: 0.3229\n",
      "Episode: 84 Total reward: 10.0 Training loss: 12.4144 Explore P: 0.3197\n",
      "Episode: 85 Total reward: 8.0 Training loss: 18.0735 Explore P: 0.3172\n",
      "Episode: 86 Total reward: 11.0 Training loss: 17.5992 Explore P: 0.3138\n",
      "Episode: 87 Total reward: 10.0 Training loss: 10.8784 Explore P: 0.3107\n",
      "Episode: 88 Total reward: 11.0 Training loss: 8.1976 Explore P: 0.3074\n",
      "Episode: 89 Total reward: 10.0 Training loss: 17.6839 Explore P: 0.3044\n",
      "Episode: 90 Total reward: 10.0 Training loss: 16.7748 Explore P: 0.3014\n",
      "Episode: 91 Total reward: 12.0 Training loss: 14.5581 Explore P: 0.2979\n",
      "Episode: 92 Total reward: 12.0 Training loss: 21.8447 Explore P: 0.2944\n",
      "Episode: 93 Total reward: 10.0 Training loss: 21.0201 Explore P: 0.2915\n",
      "Episode: 94 Total reward: 11.0 Training loss: 11.2204 Explore P: 0.2884\n",
      "Episode: 95 Total reward: 12.0 Training loss: 16.4220 Explore P: 0.2850\n",
      "Episode: 96 Total reward: 9.0 Training loss: 11.2400 Explore P: 0.2825\n",
      "Episode: 97 Total reward: 10.0 Training loss: 17.5417 Explore P: 0.2797\n",
      "Episode: 98 Total reward: 16.0 Training loss: 23.4014 Explore P: 0.2754\n",
      "Episode: 99 Total reward: 14.0 Training loss: 10.9081 Explore P: 0.2716\n",
      "Episode: 100 Total reward: 9.0 Training loss: 13.1570 Explore P: 0.2692\n",
      "Episode: 101 Total reward: 11.0 Training loss: 13.9020 Explore P: 0.2663\n",
      "Episode: 102 Total reward: 11.0 Training loss: 12.0310 Explore P: 0.2635\n",
      "Episode: 103 Total reward: 8.0 Training loss: 18.7885 Explore P: 0.2614\n",
      "Episode: 104 Total reward: 11.0 Training loss: 16.7490 Explore P: 0.2586\n",
      "Episode: 105 Total reward: 10.0 Training loss: 24.8480 Explore P: 0.2561\n",
      "Episode: 106 Total reward: 8.0 Training loss: 12.0997 Explore P: 0.2541\n",
      "Episode: 107 Total reward: 11.0 Training loss: 12.9135 Explore P: 0.2513\n",
      "Episode: 108 Total reward: 9.0 Training loss: 14.2785 Explore P: 0.2491\n",
      "Episode: 109 Total reward: 10.0 Training loss: 19.9207 Explore P: 0.2467\n",
      "Episode: 110 Total reward: 10.0 Training loss: 20.3764 Explore P: 0.2443\n",
      "Episode: 111 Total reward: 10.0 Training loss: 12.7020 Explore P: 0.2419\n",
      "Episode: 112 Total reward: 10.0 Training loss: 13.6103 Explore P: 0.2396\n",
      "Episode: 113 Total reward: 9.0 Training loss: 20.1730 Explore P: 0.2375\n",
      "Episode: 114 Total reward: 9.0 Training loss: 10.6944 Explore P: 0.2354\n",
      "Episode: 115 Total reward: 10.0 Training loss: 17.4022 Explore P: 0.2331\n",
      "Episode: 116 Total reward: 11.0 Training loss: 11.8907 Explore P: 0.2306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 117 Total reward: 10.0 Training loss: 9.9776 Explore P: 0.2284\n",
      "Episode: 118 Total reward: 11.0 Training loss: 12.1248 Explore P: 0.2259\n",
      "Episode: 119 Total reward: 11.0 Training loss: 17.2409 Explore P: 0.2235\n",
      "Episode: 120 Total reward: 10.0 Training loss: 17.6079 Explore P: 0.2213\n",
      "Episode: 121 Total reward: 12.0 Training loss: 19.4594 Explore P: 0.2187\n",
      "Episode: 122 Total reward: 13.0 Training loss: 19.2509 Explore P: 0.2160\n",
      "Episode: 123 Total reward: 9.0 Training loss: 13.0848 Explore P: 0.2141\n",
      "Episode: 124 Total reward: 9.0 Training loss: 15.2715 Explore P: 0.2122\n",
      "Episode: 125 Total reward: 10.0 Training loss: 19.9212 Explore P: 0.2102\n",
      "Episode: 126 Total reward: 10.0 Training loss: 22.8229 Explore P: 0.2081\n",
      "Episode: 127 Total reward: 14.0 Training loss: 9.0071 Explore P: 0.2053\n",
      "Episode: 128 Total reward: 13.0 Training loss: 9.8750 Explore P: 0.2027\n",
      "Episode: 129 Total reward: 10.0 Training loss: 10.7672 Explore P: 0.2007\n",
      "Episode: 130 Total reward: 11.0 Training loss: 12.5889 Explore P: 0.1986\n",
      "Episode: 131 Total reward: 9.0 Training loss: 15.4610 Explore P: 0.1969\n",
      "Episode: 132 Total reward: 10.0 Training loss: 14.3373 Explore P: 0.1949\n",
      "Episode: 133 Total reward: 9.0 Training loss: 10.0293 Explore P: 0.1932\n",
      "Episode: 134 Total reward: 8.0 Training loss: 20.7412 Explore P: 0.1917\n",
      "Episode: 135 Total reward: 11.0 Training loss: 11.1447 Explore P: 0.1897\n",
      "Episode: 136 Total reward: 9.0 Training loss: 8.3635 Explore P: 0.1880\n",
      "Episode: 137 Total reward: 8.0 Training loss: 8.0199 Explore P: 0.1866\n",
      "Episode: 138 Total reward: 9.0 Training loss: 10.6011 Explore P: 0.1850\n",
      "Episode: 139 Total reward: 9.0 Training loss: 12.9131 Explore P: 0.1833\n",
      "Episode: 140 Total reward: 12.0 Training loss: 18.0928 Explore P: 0.1812\n",
      "Episode: 141 Total reward: 8.0 Training loss: 14.8184 Explore P: 0.1798\n",
      "Episode: 142 Total reward: 11.0 Training loss: 10.4509 Explore P: 0.1779\n",
      "Episode: 143 Total reward: 10.0 Training loss: 15.7722 Explore P: 0.1762\n",
      "Episode: 144 Total reward: 9.0 Training loss: 7.9296 Explore P: 0.1747\n",
      "Episode: 145 Total reward: 8.0 Training loss: 16.6086 Explore P: 0.1733\n",
      "Episode: 146 Total reward: 8.0 Training loss: 11.8961 Explore P: 0.1720\n",
      "Episode: 147 Total reward: 13.0 Training loss: 9.2808 Explore P: 0.1698\n",
      "Episode: 148 Total reward: 12.0 Training loss: 12.6954 Explore P: 0.1678\n",
      "Episode: 149 Total reward: 10.0 Training loss: 16.1991 Explore P: 0.1662\n",
      "Episode: 150 Total reward: 8.0 Training loss: 15.3235 Explore P: 0.1649\n",
      "Episode: 151 Total reward: 11.0 Training loss: 11.0679 Explore P: 0.1632\n",
      "Episode: 152 Total reward: 9.0 Training loss: 13.2841 Explore P: 0.1618\n",
      "Episode: 153 Total reward: 12.0 Training loss: 12.9342 Explore P: 0.1599\n",
      "Episode: 154 Total reward: 10.0 Training loss: 9.1154 Explore P: 0.1584\n",
      "Episode: 155 Total reward: 9.0 Training loss: 8.6253 Explore P: 0.1570\n",
      "Episode: 156 Total reward: 10.0 Training loss: 11.6962 Explore P: 0.1555\n",
      "Episode: 157 Total reward: 9.0 Training loss: 9.9516 Explore P: 0.1541\n",
      "Episode: 158 Total reward: 11.0 Training loss: 7.1534 Explore P: 0.1525\n",
      "Episode: 159 Total reward: 11.0 Training loss: 6.9746 Explore P: 0.1509\n",
      "Episode: 160 Total reward: 9.0 Training loss: 10.0901 Explore P: 0.1496\n",
      "Episode: 161 Total reward: 9.0 Training loss: 9.3690 Explore P: 0.1483\n",
      "Episode: 162 Total reward: 9.0 Training loss: 14.6558 Explore P: 0.1470\n",
      "Episode: 163 Total reward: 10.0 Training loss: 13.0047 Explore P: 0.1456\n",
      "Episode: 164 Total reward: 9.0 Training loss: 9.0969 Explore P: 0.1443\n",
      "Episode: 165 Total reward: 10.0 Training loss: 6.2133 Explore P: 0.1429\n",
      "Episode: 166 Total reward: 12.0 Training loss: 4.8328 Explore P: 0.1413\n",
      "Episode: 167 Total reward: 9.0 Training loss: 8.5327 Explore P: 0.1401\n",
      "Episode: 168 Total reward: 9.0 Training loss: 8.7632 Explore P: 0.1389\n",
      "Episode: 169 Total reward: 10.0 Training loss: 8.0846 Explore P: 0.1375\n",
      "Episode: 170 Total reward: 12.0 Training loss: 11.6433 Explore P: 0.1359\n",
      "Episode: 171 Total reward: 9.0 Training loss: 6.0739 Explore P: 0.1348\n",
      "Episode: 172 Total reward: 11.0 Training loss: 8.4281 Explore P: 0.1333\n",
      "Episode: 173 Total reward: 9.0 Training loss: 6.2108 Explore P: 0.1322\n",
      "Episode: 174 Total reward: 11.0 Training loss: 9.8549 Explore P: 0.1308\n",
      "Episode: 175 Total reward: 13.0 Training loss: 7.1999 Explore P: 0.1292\n",
      "Episode: 176 Total reward: 11.0 Training loss: 13.3878 Explore P: 0.1278\n",
      "Episode: 177 Total reward: 13.0 Training loss: 4.9771 Explore P: 0.1262\n",
      "Episode: 178 Total reward: 12.0 Training loss: 9.2102 Explore P: 0.1248\n",
      "Episode: 179 Total reward: 13.0 Training loss: 10.1273 Explore P: 0.1232\n",
      "Episode: 180 Total reward: 12.0 Training loss: 6.2164 Explore P: 0.1218\n",
      "Episode: 181 Total reward: 16.0 Training loss: 5.1618 Explore P: 0.1200\n",
      "Episode: 182 Total reward: 14.0 Training loss: 6.4474 Explore P: 0.1184\n",
      "Episode: 183 Total reward: 13.0 Training loss: 6.7326 Explore P: 0.1169\n",
      "Episode: 184 Total reward: 15.0 Training loss: 11.0475 Explore P: 0.1152\n",
      "Episode: 185 Total reward: 15.0 Training loss: 10.2584 Explore P: 0.1136\n",
      "Episode: 186 Total reward: 13.0 Training loss: 8.8633 Explore P: 0.1122\n",
      "Episode: 187 Total reward: 12.0 Training loss: 7.2670 Explore P: 0.1109\n",
      "Episode: 188 Total reward: 14.0 Training loss: 8.9165 Explore P: 0.1095\n",
      "Episode: 189 Total reward: 17.0 Training loss: 8.3643 Explore P: 0.1077\n",
      "Episode: 190 Total reward: 19.0 Training loss: 7.0054 Explore P: 0.1058\n",
      "Episode: 191 Total reward: 17.0 Training loss: 7.4533 Explore P: 0.1041\n",
      "Episode: 192 Total reward: 16.0 Training loss: 6.0364 Explore P: 0.1025\n",
      "Episode: 193 Total reward: 17.0 Training loss: 11.8925 Explore P: 0.1008\n",
      "Episode: 194 Total reward: 15.0 Training loss: 9.7667 Explore P: 0.0994\n",
      "Episode: 195 Total reward: 17.0 Training loss: 4.8721 Explore P: 0.0978\n",
      "Episode: 196 Total reward: 13.0 Training loss: 5.3860 Explore P: 0.0966\n",
      "Episode: 197 Total reward: 13.0 Training loss: 3.7900 Explore P: 0.0954\n",
      "Episode: 198 Total reward: 200.0 Training loss: 10.6048 Explore P: 0.0791\n",
      "Episode: 199 Total reward: 121.0 Training loss: 9.6720 Explore P: 0.0706\n",
      "Episode: 200 Total reward: 82.0 Training loss: 9.2454 Explore P: 0.0654\n",
      "Episode: 201 Total reward: 86.0 Training loss: 8.0141 Explore P: 0.0605\n",
      "Episode: 202 Total reward: 102.0 Training loss: 5.7399 Explore P: 0.0551\n",
      "Episode: 203 Total reward: 76.0 Training loss: 12.1196 Explore P: 0.0514\n",
      "Episode: 204 Total reward: 30.0 Training loss: 7.0642 Explore P: 0.0500\n",
      "Episode: 205 Total reward: 25.0 Training loss: 14.5404 Explore P: 0.0489\n",
      "Episode: 206 Total reward: 19.0 Training loss: 12.3209 Explore P: 0.0481\n",
      "Episode: 207 Total reward: 30.0 Training loss: 6.0873 Explore P: 0.0468\n",
      "Episode: 208 Total reward: 28.0 Training loss: 17.6663 Explore P: 0.0457\n",
      "Episode: 209 Total reward: 59.0 Training loss: 17.0971 Explore P: 0.0433\n",
      "Episode: 210 Total reward: 41.0 Training loss: 13.0118 Explore P: 0.0418\n",
      "Episode: 211 Total reward: 27.0 Training loss: 10.0576 Explore P: 0.0408\n",
      "Episode: 212 Total reward: 34.0 Training loss: 13.0876 Explore P: 0.0396\n",
      "Episode: 213 Total reward: 25.0 Training loss: 7.7104 Explore P: 0.0388\n",
      "Episode: 214 Total reward: 25.0 Training loss: 14.0041 Explore P: 0.0379\n",
      "Episode: 215 Total reward: 36.0 Training loss: 7.3988 Explore P: 0.0368\n",
      "Episode: 216 Total reward: 29.0 Training loss: 20.6557 Explore P: 0.0359\n",
      "Episode: 217 Total reward: 19.0 Training loss: 14.5873 Explore P: 0.0353\n",
      "Episode: 218 Total reward: 29.0 Training loss: 23.5882 Explore P: 0.0344\n",
      "Episode: 219 Total reward: 21.0 Training loss: 26.2736 Explore P: 0.0338\n",
      "Episode: 220 Total reward: 22.0 Training loss: 10.0711 Explore P: 0.0332\n",
      "Episode: 221 Total reward: 22.0 Training loss: 23.1459 Explore P: 0.0326\n",
      "Episode: 222 Total reward: 27.0 Training loss: 7.6323 Explore P: 0.0318\n",
      "Episode: 223 Total reward: 24.0 Training loss: 27.9107 Explore P: 0.0312\n",
      "Episode: 224 Total reward: 34.0 Training loss: 5.8878 Explore P: 0.0303\n",
      "Episode: 225 Total reward: 43.0 Training loss: 21.0281 Explore P: 0.0293\n",
      "Episode: 226 Total reward: 36.0 Training loss: 25.8119 Explore P: 0.0284\n",
      "Episode: 227 Total reward: 57.0 Training loss: 15.8006 Explore P: 0.0271\n",
      "Episode: 228 Total reward: 23.0 Training loss: 8.2747 Explore P: 0.0266\n",
      "Episode: 229 Total reward: 47.0 Training loss: 19.7175 Explore P: 0.0256\n",
      "Episode: 230 Total reward: 34.0 Training loss: 10.2384 Explore P: 0.0249\n",
      "Episode: 231 Total reward: 27.0 Training loss: 5.1913 Explore P: 0.0244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 232 Total reward: 33.0 Training loss: 17.5609 Explore P: 0.0238\n",
      "Episode: 233 Total reward: 28.0 Training loss: 16.2816 Explore P: 0.0232\n",
      "Episode: 234 Total reward: 40.0 Training loss: 22.0723 Explore P: 0.0225\n",
      "Episode: 235 Total reward: 32.0 Training loss: 20.9252 Explore P: 0.0220\n",
      "Episode: 236 Total reward: 27.0 Training loss: 33.0417 Explore P: 0.0215\n",
      "Episode: 237 Total reward: 34.0 Training loss: 20.2757 Explore P: 0.0210\n",
      "Episode: 238 Total reward: 23.0 Training loss: 13.0899 Explore P: 0.0206\n",
      "Episode: 239 Total reward: 27.0 Training loss: 14.7782 Explore P: 0.0202\n",
      "Episode: 240 Total reward: 33.0 Training loss: 24.2632 Explore P: 0.0197\n",
      "Episode: 241 Total reward: 33.0 Training loss: 13.5706 Explore P: 0.0192\n",
      "Episode: 242 Total reward: 28.0 Training loss: 26.1938 Explore P: 0.0188\n",
      "Episode: 243 Total reward: 30.0 Training loss: 32.5437 Explore P: 0.0184\n",
      "Episode: 244 Total reward: 28.0 Training loss: 4.4608 Explore P: 0.0180\n",
      "Episode: 245 Total reward: 45.0 Training loss: 22.3832 Explore P: 0.0175\n",
      "Episode: 246 Total reward: 44.0 Training loss: 27.0217 Explore P: 0.0169\n",
      "Episode: 247 Total reward: 36.0 Training loss: 9.3615 Explore P: 0.0165\n",
      "Episode: 248 Total reward: 40.0 Training loss: 35.4234 Explore P: 0.0161\n",
      "Episode: 249 Total reward: 34.0 Training loss: 20.6561 Explore P: 0.0157\n",
      "Episode: 250 Total reward: 26.0 Training loss: 16.0894 Explore P: 0.0154\n",
      "Episode: 251 Total reward: 34.0 Training loss: 14.5806 Explore P: 0.0151\n",
      "Episode: 252 Total reward: 58.0 Training loss: 10.9779 Explore P: 0.0145\n",
      "Episode: 253 Total reward: 30.0 Training loss: 10.3335 Explore P: 0.0142\n",
      "Episode: 254 Total reward: 40.0 Training loss: 7.7331 Explore P: 0.0139\n",
      "Episode: 255 Total reward: 58.0 Training loss: 10.0001 Explore P: 0.0134\n",
      "Episode: 256 Total reward: 71.0 Training loss: 28.3431 Explore P: 0.0128\n",
      "Episode: 257 Total reward: 60.0 Training loss: 15.9992 Explore P: 0.0123\n",
      "Episode: 258 Total reward: 31.0 Training loss: 30.9553 Explore P: 0.0121\n",
      "Episode: 259 Total reward: 52.0 Training loss: 13.6156 Explore P: 0.0118\n",
      "Episode: 260 Total reward: 29.0 Training loss: 36.1851 Explore P: 0.0116\n",
      "Episode: 261 Total reward: 81.0 Training loss: 15.9767 Explore P: 0.0110\n",
      "Episode: 262 Total reward: 55.0 Training loss: 42.2395 Explore P: 0.0107\n",
      "Episode: 263 Total reward: 44.0 Training loss: 14.5870 Explore P: 0.0105\n",
      "Episode: 264 Total reward: 48.0 Training loss: 19.2393 Explore P: 0.0102\n",
      "Episode: 265 Total reward: 32.0 Training loss: 15.8753 Explore P: 0.0101\n",
      "Episode: 266 Total reward: 68.0 Training loss: 23.8190 Explore P: 0.0097\n",
      "Episode: 267 Total reward: 59.0 Training loss: 26.3635 Explore P: 0.0095\n",
      "Episode: 268 Total reward: 69.0 Training loss: 20.8563 Explore P: 0.0092\n",
      "Episode: 269 Total reward: 34.0 Training loss: 16.3126 Explore P: 0.0090\n",
      "Episode: 270 Total reward: 52.0 Training loss: 8.6200 Explore P: 0.0088\n",
      "Episode: 271 Total reward: 42.0 Training loss: 21.2783 Explore P: 0.0087\n",
      "Episode: 272 Total reward: 54.0 Training loss: 28.5983 Explore P: 0.0085\n",
      "Episode: 273 Total reward: 85.0 Training loss: 32.6106 Explore P: 0.0082\n",
      "Episode: 274 Total reward: 53.0 Training loss: 11.5582 Explore P: 0.0080\n",
      "Episode: 275 Total reward: 61.0 Training loss: 23.1011 Explore P: 0.0078\n",
      "Episode: 276 Total reward: 39.0 Training loss: 15.8979 Explore P: 0.0077\n",
      "Episode: 277 Total reward: 42.0 Training loss: 18.8001 Explore P: 0.0076\n",
      "Episode: 278 Total reward: 53.0 Training loss: 11.9048 Explore P: 0.0075\n",
      "Episode: 279 Total reward: 75.0 Training loss: 10.3045 Explore P: 0.0073\n",
      "Episode: 280 Total reward: 74.0 Training loss: 41.4293 Explore P: 0.0071\n",
      "Episode: 281 Total reward: 42.0 Training loss: 15.3158 Explore P: 0.0071\n",
      "Episode: 282 Total reward: 39.0 Training loss: 32.5226 Explore P: 0.0070\n",
      "Episode: 283 Total reward: 49.0 Training loss: 2.7892 Explore P: 0.0069\n",
      "Episode: 284 Total reward: 48.0 Training loss: 41.1886 Explore P: 0.0068\n",
      "Episode: 285 Total reward: 53.0 Training loss: 27.5171 Explore P: 0.0067\n",
      "Episode: 286 Total reward: 40.0 Training loss: 25.7994 Explore P: 0.0066\n",
      "Episode: 287 Total reward: 60.0 Training loss: 21.0074 Explore P: 0.0065\n",
      "Episode: 288 Total reward: 75.0 Training loss: 23.6599 Explore P: 0.0064\n",
      "Episode: 289 Total reward: 44.0 Training loss: 10.4227 Explore P: 0.0064\n",
      "Episode: 290 Total reward: 50.0 Training loss: 6.5702 Explore P: 0.0063\n",
      "Episode: 291 Total reward: 97.0 Training loss: 6.9907 Explore P: 0.0062\n",
      "Episode: 292 Total reward: 93.0 Training loss: 15.4262 Explore P: 0.0061\n",
      "Episode: 293 Total reward: 73.0 Training loss: 14.1409 Explore P: 0.0060\n",
      "Episode: 294 Total reward: 48.0 Training loss: 18.1908 Explore P: 0.0060\n",
      "Episode: 295 Total reward: 46.0 Training loss: 13.3180 Explore P: 0.0059\n",
      "Episode: 296 Total reward: 50.0 Training loss: 7.1611 Explore P: 0.0059\n",
      "Episode: 297 Total reward: 56.0 Training loss: 5.8034 Explore P: 0.0058\n",
      "Episode: 298 Total reward: 55.0 Training loss: 20.9748 Explore P: 0.0058\n",
      "Episode: 299 Total reward: 52.0 Training loss: 51.4017 Explore P: 0.0057\n",
      "Episode: 300 Total reward: 59.0 Training loss: 25.9408 Explore P: 0.0057\n",
      "Episode: 301 Total reward: 52.0 Training loss: 16.7364 Explore P: 0.0057\n",
      "Episode: 302 Total reward: 134.0 Training loss: 12.3304 Explore P: 0.0056\n",
      "Episode: 303 Total reward: 60.0 Training loss: 11.5821 Explore P: 0.0055\n",
      "Episode: 304 Total reward: 74.0 Training loss: 20.1803 Explore P: 0.0055\n",
      "Episode: 305 Total reward: 58.0 Training loss: 8.1939 Explore P: 0.0055\n",
      "Episode: 306 Total reward: 51.0 Training loss: 9.2312 Explore P: 0.0055\n",
      "Episode: 307 Total reward: 54.0 Training loss: 47.3185 Explore P: 0.0054\n",
      "Episode: 308 Total reward: 73.0 Training loss: 31.4256 Explore P: 0.0054\n",
      "Episode: 309 Total reward: 70.0 Training loss: 8.7848 Explore P: 0.0054\n",
      "Episode: 310 Total reward: 200.0 Training loss: 7.1692 Explore P: 0.0053\n",
      "Episode: 311 Total reward: 81.0 Training loss: 42.8735 Explore P: 0.0053\n",
      "Episode: 312 Total reward: 86.0 Training loss: 16.6688 Explore P: 0.0053\n",
      "Episode: 313 Total reward: 74.0 Training loss: 25.9570 Explore P: 0.0052\n",
      "Episode: 314 Total reward: 116.0 Training loss: 4.9013 Explore P: 0.0052\n",
      "Episode: 315 Total reward: 120.0 Training loss: 19.9475 Explore P: 0.0052\n",
      "Episode: 316 Total reward: 107.0 Training loss: 22.4742 Explore P: 0.0052\n",
      "Episode: 317 Total reward: 105.0 Training loss: 9.2207 Explore P: 0.0052\n",
      "Episode: 318 Total reward: 100.0 Training loss: 11.5254 Explore P: 0.0051\n",
      "Episode: 319 Total reward: 128.0 Training loss: 12.0572 Explore P: 0.0051\n",
      "Episode: 320 Total reward: 138.0 Training loss: 9.8751 Explore P: 0.0051\n",
      "Episode: 321 Total reward: 104.0 Training loss: 18.1850 Explore P: 0.0051\n",
      "Episode: 322 Total reward: 108.0 Training loss: 10.8423 Explore P: 0.0051\n",
      "Episode: 323 Total reward: 120.0 Training loss: 17.7228 Explore P: 0.0051\n",
      "Episode: 324 Total reward: 160.0 Training loss: 43.6133 Explore P: 0.0051\n",
      "Episode: 325 Total reward: 127.0 Training loss: 11.2702 Explore P: 0.0051\n",
      "Episode: 326 Total reward: 116.0 Training loss: 26.9866 Explore P: 0.0051\n",
      "Episode: 327 Total reward: 121.0 Training loss: 2.0691 Explore P: 0.0050\n",
      "Episode: 328 Total reward: 158.0 Training loss: 12.4253 Explore P: 0.0050\n",
      "Episode: 329 Total reward: 188.0 Training loss: 20.5356 Explore P: 0.0050\n",
      "Episode: 330 Total reward: 172.0 Training loss: 4.7283 Explore P: 0.0050\n",
      "Episode: 331 Total reward: 143.0 Training loss: 22.4380 Explore P: 0.0050\n",
      "Episode: 332 Total reward: 155.0 Training loss: 59.0346 Explore P: 0.0050\n",
      "Episode: 333 Total reward: 183.0 Training loss: 20.4334 Explore P: 0.0050\n",
      "Episode: 334 Total reward: 157.0 Training loss: 6.9866 Explore P: 0.0050\n",
      "Episode: 335 Total reward: 181.0 Training loss: 13.4945 Explore P: 0.0050\n",
      "Episode: 336 Total reward: 191.0 Training loss: 1.7869 Explore P: 0.0050\n",
      "Episode: 337 Total reward: 169.0 Training loss: 25.8416 Explore P: 0.0050\n",
      "Episode: 338 Total reward: 194.0 Training loss: 2.5312 Explore P: 0.0050\n",
      "Episode: 339 Total reward: 200.0 Training loss: 0.9589 Explore P: 0.0050\n",
      "Episode: 340 Total reward: 200.0 Training loss: 16.9377 Explore P: 0.0050\n",
      "Episode: 341 Total reward: 200.0 Training loss: 1.2096 Explore P: 0.0050\n",
      "Episode: 342 Total reward: 200.0 Training loss: 88.9678 Explore P: 0.0050\n",
      "Episode: 343 Total reward: 200.0 Training loss: 10.5599 Explore P: 0.0050\n",
      "Episode: 344 Total reward: 200.0 Training loss: 1.4212 Explore P: 0.0050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 345 Total reward: 200.0 Training loss: 74.7182 Explore P: 0.0050\n",
      "Episode: 346 Total reward: 200.0 Training loss: 21.9143 Explore P: 0.0050\n",
      "Episode: 347 Total reward: 200.0 Training loss: 3.2540 Explore P: 0.0050\n",
      "Episode: 348 Total reward: 200.0 Training loss: 124.3797 Explore P: 0.0050\n",
      "Episode: 349 Total reward: 200.0 Training loss: 27.9767 Explore P: 0.0050\n",
      "Episode: 350 Total reward: 200.0 Training loss: 130.9413 Explore P: 0.0050\n",
      "Episode: 351 Total reward: 200.0 Training loss: 26.4631 Explore P: 0.0050\n",
      "Episode: 352 Total reward: 199.0 Training loss: 40.8419 Explore P: 0.0050\n",
      "Episode: 353 Total reward: 200.0 Training loss: 3.8413 Explore P: 0.0050\n",
      "Episode: 354 Total reward: 188.0 Training loss: 4.7707 Explore P: 0.0050\n",
      "Episode: 355 Total reward: 187.0 Training loss: 29.4155 Explore P: 0.0050\n",
      "Episode: 356 Total reward: 200.0 Training loss: 3.4380 Explore P: 0.0050\n",
      "Episode: 357 Total reward: 200.0 Training loss: 3.7827 Explore P: 0.0050\n",
      "Episode: 358 Total reward: 200.0 Training loss: 2.9771 Explore P: 0.0050\n",
      "Episode: 359 Total reward: 200.0 Training loss: 139.5202 Explore P: 0.0050\n",
      "Episode: 360 Total reward: 200.0 Training loss: 3.8377 Explore P: 0.0050\n",
      "Episode: 361 Total reward: 200.0 Training loss: 47.9412 Explore P: 0.0050\n",
      "Episode: 362 Total reward: 200.0 Training loss: 2.6721 Explore P: 0.0050\n",
      "Episode: 363 Total reward: 193.0 Training loss: 39.8540 Explore P: 0.0050\n",
      "Episode: 364 Total reward: 200.0 Training loss: 61.7515 Explore P: 0.0050\n",
      "Episode: 365 Total reward: 200.0 Training loss: 61.0773 Explore P: 0.0050\n",
      "Episode: 366 Total reward: 187.0 Training loss: 76.1802 Explore P: 0.0050\n",
      "Episode: 367 Total reward: 183.0 Training loss: 195.8294 Explore P: 0.0050\n",
      "Episode: 368 Total reward: 166.0 Training loss: 3.2912 Explore P: 0.0050\n",
      "Episode: 369 Total reward: 190.0 Training loss: 72.4452 Explore P: 0.0050\n",
      "Episode: 370 Total reward: 155.0 Training loss: 319.3233 Explore P: 0.0050\n",
      "Episode: 371 Total reward: 148.0 Training loss: 2.8311 Explore P: 0.0050\n",
      "Episode: 372 Total reward: 155.0 Training loss: 2.8185 Explore P: 0.0050\n",
      "Episode: 373 Total reward: 167.0 Training loss: 2.3114 Explore P: 0.0050\n",
      "Episode: 374 Total reward: 149.0 Training loss: 102.9249 Explore P: 0.0050\n",
      "Episode: 375 Total reward: 144.0 Training loss: 1.9973 Explore P: 0.0050\n",
      "Episode: 376 Total reward: 161.0 Training loss: 2.4874 Explore P: 0.0050\n",
      "Episode: 377 Total reward: 160.0 Training loss: 1.9441 Explore P: 0.0050\n",
      "Episode: 378 Total reward: 154.0 Training loss: 1.2175 Explore P: 0.0050\n",
      "Episode: 379 Total reward: 165.0 Training loss: 1.8803 Explore P: 0.0050\n",
      "Episode: 380 Total reward: 152.0 Training loss: 1.3124 Explore P: 0.0050\n",
      "Episode: 381 Total reward: 169.0 Training loss: 1.3550 Explore P: 0.0050\n",
      "Episode: 382 Total reward: 149.0 Training loss: 91.1712 Explore P: 0.0050\n",
      "Episode: 383 Total reward: 157.0 Training loss: 0.9592 Explore P: 0.0050\n",
      "Episode: 384 Total reward: 182.0 Training loss: 1.1025 Explore P: 0.0050\n",
      "Episode: 385 Total reward: 148.0 Training loss: 1.0728 Explore P: 0.0050\n",
      "Episode: 386 Total reward: 191.0 Training loss: 22.5630 Explore P: 0.0050\n",
      "Episode: 387 Total reward: 190.0 Training loss: 0.9900 Explore P: 0.0050\n",
      "Episode: 388 Total reward: 200.0 Training loss: 15.6700 Explore P: 0.0050\n",
      "Episode: 389 Total reward: 161.0 Training loss: 0.9950 Explore P: 0.0050\n",
      "Episode: 390 Total reward: 200.0 Training loss: 0.8444 Explore P: 0.0050\n",
      "Episode: 391 Total reward: 196.0 Training loss: 0.7959 Explore P: 0.0050\n",
      "Episode: 392 Total reward: 190.0 Training loss: 0.7453 Explore P: 0.0050\n",
      "Episode: 393 Total reward: 200.0 Training loss: 0.8769 Explore P: 0.0050\n",
      "Episode: 394 Total reward: 200.0 Training loss: 0.7039 Explore P: 0.0050\n",
      "Episode: 395 Total reward: 200.0 Training loss: 0.4716 Explore P: 0.0050\n",
      "Episode: 396 Total reward: 200.0 Training loss: 0.6538 Explore P: 0.0050\n",
      "Episode: 397 Total reward: 200.0 Training loss: 2.5082 Explore P: 0.0050\n",
      "Episode: 398 Total reward: 200.0 Training loss: 7.0192 Explore P: 0.0050\n",
      "Episode: 399 Total reward: 200.0 Training loss: 0.4903 Explore P: 0.0050\n"
     ]
    }
   ],
   "source": [
    "rewards_list = []\n",
    "\n",
    "step = 0\n",
    "for ep in range(train_episodes):\n",
    "    total_reward = 0\n",
    "    t = 0\n",
    "    state = env.reset()  # Reset and get initial state\n",
    "    while t < max_steps:\n",
    "        step += 1\n",
    "        # Uncomment this next line to watch the training\n",
    "        # env.render() \n",
    "            \n",
    "        # Explore or exploit\n",
    "        explore_p = explore_stop + (explore_start - explore_stop)*np.exp(-decay_rate*step) \n",
    "        if explore_p > np.random.rand():\n",
    "            # Make a random action\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            # Get action from Q-network\n",
    "            Qs = mainQN(np.resize(state, (1, state_size)).astype(np.float32))\n",
    "            action = np.argmax(Qs)\n",
    "            \n",
    "        # Take action, get new state and reward\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "    \n",
    "        total_reward += reward\n",
    "           \n",
    "        if done:\n",
    "            # Episode ends, so no next state\n",
    "            next_state = np.zeros(state.shape)\n",
    "            t = max_steps\n",
    "                \n",
    "            print('Episode: {}'.format(ep), 'Total reward: {}'.format(total_reward),\n",
    "                  'Training loss: {:.4f}'.format(loss), 'Explore P: {:.4f}'.format(explore_p))\n",
    "            rewards_list.append((ep, total_reward))\n",
    "                \n",
    "            # Add experience to memory\n",
    "            memory.add((state, action, reward, next_state))\n",
    "        else:\n",
    "            # Add experience to memory\n",
    "            memory.add((state, action, reward, next_state))\n",
    "            state = next_state\n",
    "            t += 1\n",
    "            \n",
    "        # Sample mini-batch from memory\n",
    "        batch = memory.sample(batch_size)\n",
    "        states = np.array([each[0] for each in batch])\n",
    "        actions = np.array([each[1] for each in batch])\n",
    "        rewards = np.array([each[2] for each in batch])\n",
    "        next_states = np.array([each[3] for each in batch])\n",
    "            \n",
    "        # Train network           \n",
    "        target_Qs = mainQN(next_states.astype(np.float32)).numpy()\n",
    "            \n",
    "        # Set target_Qs to 0 for states where episode ends\n",
    "        episode_ends = (next_states == np.zeros(states[0].shape)).all(axis=1)\n",
    "        target_Qs[episode_ends] = (0., 0.)\n",
    "            \n",
    "        # Compute target Q values   \n",
    "        targets = rewards + gamma * np.max(target_Qs, axis=1)  # Max: Q-learning is off-policy & greedy \n",
    "\n",
    "        # Gradient-based update\n",
    "        loss, gradients = compute_gradient(states.astype(np.float32), actions, targets.astype(np.float32))\n",
    "        optimizer.apply_gradients(zip(gradients, mainQN.trainable_variables))\n",
    "\n",
    "log_path = \"/tmp/deep_Q_network\"\n",
    "mainQN.save_weights(log_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate learning process and final policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving average for smoothing plot\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Total Reward')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "eps, rews = np.array(rewards_list).T\n",
    "smoothed_rews = running_mean(rews, 10)\n",
    "plt.plot(eps[-len(smoothed_rews):], smoothed_rews)\n",
    "plt.plot(eps, rews, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial state: [ 0.03519044 -0.04169344 -0.01145805 -0.02859688]\n",
      "number of steps: 200.0\n",
      "initial state: [ 0.04684447 -0.01760597 -0.04280387 -0.00931263]\n",
      "number of steps: 200.0\n",
      "initial state: [-0.04736928 -0.04007403  0.02739608  0.00650904]\n",
      "number of steps: 200.0\n",
      "initial state: [ 0.01735609 -0.03771807 -0.00032947  0.0382812 ]\n",
      "number of steps: 182.0\n",
      "initial state: [-0.03718574 -0.04106284  0.04537808  0.03193044]\n",
      "number of steps: 182.0\n",
      "initial state: [-0.02367494 -0.04973627 -0.03449531 -0.02933311]\n",
      "number of steps: 200.0\n",
      "initial state: [-0.03656805  0.00683787  0.03056128 -0.00706383]\n",
      "number of steps: 195.0\n",
      "initial state: [-0.00209477 -0.0178964  -0.03209121 -0.02080103]\n",
      "number of steps: 200.0\n",
      "initial state: [ 0.04425383 -0.03174616  0.04297296 -0.04320745]\n",
      "number of steps: 159.0\n",
      "initial state: [-0.03395295 -0.04571436  0.01957676 -0.04929838]\n",
      "number of steps: 200.0\n"
     ]
    }
   ],
   "source": [
    "testQN = QNetwork(name='test', hidden_size=hidden_size)\n",
    "testQN.build(input_shape=(None, state_size))\n",
    "testQN.load_weights(log_path)\n",
    "\n",
    "test_episodes = 10\n",
    "\n",
    "for ep in range(test_episodes):\n",
    "    state = env.reset()\n",
    "    print(\"initial state:\", state)\n",
    "    R = 0\n",
    "    while True:\n",
    "        env.render() \n",
    "            \n",
    "        # Get action from Q-network\n",
    "        # Hm, the following line could be more elegant ...\n",
    "        Qs = testQN(np.float32(np.resize(state, (1, state_size))))\n",
    "        action = np.argmax(Qs)\n",
    "            \n",
    "        # Take action, get new state and reward\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        R += reward\n",
    "            \n",
    "        if done:\n",
    "            print(\"number of steps:\", R)\n",
    "            break\n",
    "        else:\n",
    "            state = next_state\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
