{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular Q-learning using a linear model in TF2\n",
    "## Christian Igel, 2019\n",
    "\n",
    "This example implements tabular Q-learning via a linear model and applies it to simple gridworlds. If you have suggestions for improvement, [let me know](mailto:igel@diku.dk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import gym\n",
    "import gym_gridworlds  # pip install gym-gridworlds\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "|S| = 48\n",
      "|A| = 4\n"
     ]
    }
   ],
   "source": [
    "# Choose either of the two environments\n",
    "env_name = 'CliffWalking-v0'\n",
    "# env_name = 'FrozenLake-v0'\n",
    "env = gym.make(env_name)  \n",
    "\n",
    "env.render()\n",
    "\n",
    "number_of_actions = env.action_space.n\n",
    "number_of_states = env.observation_space.n\n",
    "print(\"|S| =\", number_of_states)\n",
    "print(\"|A| =\", number_of_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gym.envs.toy_text.cliffwalking.CliffWalkingEnv at 0x1731b968278>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 48)]              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4)                 192       \n",
      "=================================================================\n",
      "Total params: 192\n",
      "Trainable params: 192\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define linear model\n",
    "#x = tf.keras.Input(shape=(number_of_states,), dtype=tf.float64)  # input state\n",
    "x = tf.keras.Input(shape=(number_of_states,))  # input state\n",
    "y = tf.keras.layers.Dense(number_of_actions, activation=None, use_bias=False, \n",
    "                          kernel_initializer=tf.keras.initializers.RandomUniform(0, 0.01))(x)\n",
    "#argmax_y = tf.argmax(y, 1) # best action\n",
    "\n",
    "# Instantiate model\n",
    "model = tf.keras.Model(inputs=x, outputs=y)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.1  # (initial) learning rate\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=eta)\n",
    "\n",
    "@tf.function\n",
    "def compute_gradient(x, Q_target):\n",
    "    with tf.GradientTape() as tape:\n",
    "            Q = model(x)\n",
    "            loss = tf.math.reduce_mean(tf.square(Q - Q_target))\n",
    "    return tape.gradient(loss, model.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set learning parameters\n",
    "gamma = 1. #.99  # gamma\n",
    "initial_epsilon = epsilon = 0.1  # epsilon for epsilon-greedy selection\n",
    "number_of_episodes = 2000\n",
    "max_number_of_steps = 100\n",
    "T_list = []  # list gathering maximum number of steps for each episode\n",
    "R_list = [] \n",
    "\n",
    "for i in tqdm(range(number_of_episodes)):\n",
    "    s = env.reset()  # reset environment and get first state\n",
    "    R = 0  # return (accumulated reward)\n",
    "    for t in range(max_number_of_steps):  # maximum number of steps\n",
    "        # Choose an action greedily (with e chance of random action) from the Q-network\n",
    "        Q = model(np.eye(1, number_of_states, s, dtype=np.float32))\n",
    "        a = np.argmax(Q, 1) # best action\n",
    "        if np.random.rand(1) < epsilon:\n",
    "            a[0] = env.action_space.sample()\n",
    "        # Observe new state and reward from environment\n",
    "        s_prime, r, d, _ = env.step(a[0])\n",
    "        # Compute Q' by feeding the new state into the network\n",
    "        Q_prime = model(np.eye(1, number_of_states, s_prime, dtype=np.float32))\n",
    "        # Compute maximum value of Q_prime and set  target value for chosen action\n",
    "        max_Q_prime = np.max(Q_prime)\n",
    "        Q_target = Q.numpy()\n",
    "        Q_target[0, a[0]] = r + gamma * max_Q_prime\n",
    "        # Train network using target and predicted Q values\n",
    "        gradients = compute_gradient(np.eye(1, number_of_states, s, dtype=np.float32), Q_target); \n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        R += r\n",
    "        s = s_prime\n",
    "        if d == True:  # episode ended\n",
    "            # Reduce probability of random actions over time\n",
    "            epsilon = 1./((i/50) + (1./initial_epsilon))\n",
    "            break\n",
    "    T_list.append(t)\n",
    "    R_list.append(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_target = Q.numpy()\n",
    "Q_target[0, a[0]] = 1 + 1 * max_Q_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00691037, 0.00763018, 0.0064079 , 0.00580551]], dtype=float32)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00691037, 1.0076302 , 0.0064079 , 0.00580551]], dtype=float32)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.00691037 0.00763018 0.0064079  0.00580551]], shape=(1, 4), dtype=float32)\n",
      "0.0076301834\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.00691037, 0.00763018, 0.0064079 , 0.00580551]], dtype=float32)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(Q_prime)\n",
    "print(max_Q_prime)\n",
    "Q.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = env.reset()\n",
    "np.eye(1, number_of_states, s, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=51, shape=(1, 4), dtype=float32, numpy=array([[0.00691037, 0.00763018, 0.0064079 , 0.00580551]], dtype=float32)>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = model(np.eye(1, number_of_states, s, dtype=np.float32))\n",
    "Q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25323255])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int64)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.argmax(Q, 1)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if env_name == 'FrozenLake-v0':\n",
    "    print(\"Percent of succesful episodes:\", sum(R_list)/number_of_episodes)\n",
    "plt.plot(R_list, 'g.')\n",
    "plt.show()\n",
    "plt.plot(T_list, 'b.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()\n",
    "\n",
    "weights = model.get_weights()\n",
    "\n",
    "if env_name == 'FrozenLake-v0':\n",
    "    print(\"V:\\n\", np.around(np.max(weights, 2).reshape((4,4)), decimals=1))\n",
    "    print(\"actions:\\n\", np.argmax(weights, 2).reshape((4,4)))\n",
    "if env_name == 'CliffWalking-v0':\n",
    "    print(\"V:\\n\", np.around(np.max(weights, 2).reshape((4,12)), decimals=1))\n",
    "    print(\"actions:\\n\", np.argmax(weights, 2).reshape((4,12)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
